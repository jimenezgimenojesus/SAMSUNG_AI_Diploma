---------------------------------------------------------------------
29/09/2025 Maria
--------------------------------------------------
definición de probabilidad = casos favorables / casos posibles (toda P entre 0 y 1)

discreta = finito, personas hay 1,2,3, no 1.2
		 suma de valores da 1
continua = hay infintos resultados entre uno y otro
                  se hace la integral (sumatorio de inifnitos valores)


population = no conozco todos los elementos, por tanto es un PARAMETRO aproximado
sample = conozco seguro, por tanto son ESTADÍSTICAS


LA medai devuelve el valor, pero no tiene que ser uno de los casos reales (por las dsicretas, por ejemplo, ejemplo de los dados que se tiran cuya media es 3.5 aunque el dado no tenga una cara de 3.5).


Comparativa de los años: la varianza es la misma aunque crezcamos pasado un año, pero la medai habrá crecido la parte porporcioanr del tiempo que haya transcuirrdo.

Benrouilli: probabilida de que sea o A o no A y punto.
          -  la suma de Bernoulli conduce a distirbucion binomial
          -  se mantiene constate la probabilidad del suceso
X - Bin(n, p): dsitribucion binomial uq edepemde de n y de p

---------
App = Probability Distributions, allows to select fro probability distributions.
-------



---------------------------------------------------------------------
01/10/2025: Conjoint Probability
---------------------------------------------------------------------


Varianza x: S_x^2 = sum(x_i - x.mean)/(n - 1)
Varianza y: S_y^2 = sum(...)/(n - 1)          -------> n son los pares de datos
Covarinza es el grado de dispersión entre ambasa variables (igual que la Varianza de una pero con dos)

Cob(x, y) = E((x_i - mu_x)(y_i - mu_y))
           E = expected value


Misma variablidadad cambia con unidades diferentes. Peso yh altura, en kg, g, m, cm... --> Problema de dependencia de unidades de trabajo (muy relacioenadas). Para ver la relacion hay que gacer el correlaction coefficient. --> normalizar valroes . Da relacion LINEAL entre variables (no relaicón lineal NO SIGNIFICA NO RELACIÓN). --> Variables relacioneadas por ejemplo cuadráticamente recibiran un coeficiente 0 porque no estará relacionado, A PESAR DE TENER UNA RELACIÓN CUADRÁTICA.

Ejercicio:
P(X = -1, Y = 1) = 1/3, P(X = 0, Y = 0) = 1/3, P(X = 1, Y = 1) = 3

# LA X es discreta, porque toma valores conocides, por lo que E = sum(valor * Pobabilidad_i) 
# RECUERDA: CASOS FAVORABLES / CASOS POSIBLES

Cov(X, Y) = E[X Y] - E[X]*E[Y] = 0                           		#
E[X] = -1*P(X=-1) + 0*P(X=0) + 1*P(X=1) = -1/3 + 0 + 1/3 = 0   		# Expected value de X
E[Y] = 0*P(Y = 0) + 1*P(Y = 1) = 0 + 1*(1/3 + 1/3) = 2/3                # Expected value de Y  
E[X Y] = E[X * X^2] = E[X^3] = (-1)^3*P(X = -1) + ... resto * prob = 0  # Expected value of XY                       

Como la Cov(X, Y) = 0 --> Cor(X, Y) = 0

PAra ver si son indeoendeintes: Probabilidad conjunta == Producto de Probabilidades ---> Ha de cumplirse para todos los casos!!

P(X=-1, Y=1) == P(X=-1)*P(Y=1) -> 1/3 =/= 1/3*2/3




----------
Sample: muestra pequeña de la población ---> para inferir datos de la población 
Descriptiva: de la muestra, Inferenical, de la población.

Mu es la medai de la poblaciñon y x^_ es la medai de una muestra
sigma^2 es varianza población, S^2 de muestra
sd es s en muestra y sigma en población

Mediana: deja la mitad de los datos por arriba y por debajo
Skewness: …
Fructosis: mide si los datos son más 'chatos' o 'picudos' que la normal.

Quartiles: divide los datos en 4 partes iguales (SON LA DIVISIÓN: 25, 50, 75, SÓLO 3 'CORTES')
Quantiles: n-cortes, percentil --> 99 cortes (sobre 100, %).

IQR: Q3 -Q1
length bigote = Q_i - 1.5xIQR (si esto es > que el ultimo valor, llega sólo hasta ekl ultimo valor).




1.96 ---> intervlo de confianza del 95%, cambiarlo cambia el intervalod e confianza, que tñu decides cuándo ueires más o menos confianza (más confianza == menos "errores" pero menos orecision.)
Cuanto más grande el tmañaod e muestra, más se acerca al intervalo.

---------------------------------------
EJERCICIO INTERVALO 1

n     = 15 ejecuciones
x^_   = 47.07 ms
s^2   = 2.97 ms^2
mu    = 47.8 ms     (¿?)
alpha = 0.05 --> 1.95

¿Mu = 47.8 con intervalo de 1.95?

IC(95%) = [x^_-t_(1-alpha/2)*s/sqrt(n)]
        = [47.07 - 1.761 * sqrt(2.97)/sqrt(15)]   # sacar con el alfa y y los grados de libertad el t_(...)
        = [46.1156, 48.0243]                      # Izqd y dcha

---------------------------------------
EJERCICIO INTERVALO 2

n     = 100 ejecuciones
x^_   = 778
s^2   = 193.4
sigma =                                           # Como no tengo sigma utilizo t, que se aproximará a la normal, haré normal con s, porque n es muy grande, pero no sigma
alpha = 0.05 --> 1.95

IC(95%) = [x^_-t_(1-alpha/2)*s/sqrt(n)]
        = [778 - 1.96 * sqrt(193.4)/sqrt(100)]   # sacar con el alfa y y los grados de libertad el t_(...)
        = [46.1156, 48.0243]                       # Izqd y dcha



---------------------------------------
EJERCICIO CHI-CUADRADO

pblacion = votantes
muestra = 200vot
variables:                                # Queremos comparar y ver si son independientes o no
- rango de edad
- partido de voto     --------> tabla de frecuencias observadas (O_ij)

H_0 = que sean indeoendeitnes
H_1 = que no sean independientes
Alpha = 0.05

E = O_i * O_j / muestra ----> tabla de frecuencias esperadas (E_ij)

`X` = sum(sum(O_ij - E_ij) / E_ij)                       # sumatoria de cada categoría (2 sumatorios en este caso)
grado de libertad = (cat_i - 1) * (cat_2 - 1)            # Obtener los grados de libertad con las categorías         

REAHCZAR = NO ESTAR EN EL ALPHA (EN EL INTERVALO DE CONFIANZA)






----------------------------
03/10/25
----------------------
ARBOLES DE DECISIÓN: decidir de entre todas las variables, la primera y a partir de esas las demás en importancia.

ENTROPÑIA: Homogeneidad entre los datos (con logaritmos para aproximar, con given formula)
con la misma diferencia, pero mñas datos hay mas homgeneidad.

GANANCIA INFORMACION: ENTROPIA - sum()   ->< elegir nodo primero según amyor ganancia
ELEGIR el siguiente nodo dentro de los nodos del primero viendo como les afecta el resto de variables.


----------------------------------
13/10/25
Aplicación the Supervised Model Classification
----------------------------------
Regresión: de variables ¿?, se predice categorías
Clasificación: de variables descriptivas, se predice categorías

LA MAYORÍA SON EJEMPLOS DE CLASIFICACIÓN

Modelos eneseñados: son los REPRESENTANTES DE LAS FAMÍLIAS: DISTANCIAS, PROBABILIDAD (Bayes es POBRE), REGRESIÓN LOGÍSTICA, ÁRBOLES DE DEICISIÓN, REDES NEURONALES ARTIFICIALES (MÁS ADELANTE, 1 NEURONA, MULTIPLES NEURONEAS, CAPAS INTERNAS, ...), MÁQUINA DE VECTOR DE SOPORTE, estado del arte ------> para datos tabulares: RANDOM FOREST Y XBOOST (son ensamble: conjunto de modelos)

- REGRESIÓN LINEAL: 
- REGRESIÓN LOGÍSTICA: 
- EJERCICIO: solver en LLinear REgression object, depende de los datos
            ajustar
               predice (devuelve las clases)
		confusion matrix --> HARD WAY (manually select from matrix)
                                 |-> EASY WAY
	predict_proba(devuleve vector de pertenencia de clase para cada test)
	
	cambia el thresold y calcula otra vez la confusion matrix (lanza runa curva Roth y comprobar el mejor cutoff)

- DECISION TREE: sencillos de entender, robustos para NaN, 
		ESCOGER EL PRIMER ATRIBUTO EN LOS CONJUNTOS MÁS PUROS (según gananxia, entropía)
		¡¡¡¡son muy dependeintes del datatset de entrenamiento!!!!
		CRITERIO DE PARADA
		NODOS HOJA
		NODO PURO
		PUEDE HABER ATRIBUTOS IRRELEVANTES
		SE PUEDE TRANSOFMRAR DE ARBOL DE DECISION A REGLAS
		SI EL NODO FINAL NO ES NODO PURO (UNA SOLA CLASE), SE PARA POR OTRO CRITERIO (profundidad máxima, por ejemplo, ) -> CLASE MAYORITARIA
		SCIKIT, NO TRABAJ CON DTOS CATEG´ROCOS: LOS PASA ONE-HOT/ NUMEROS
		GRÄFICA CON PARALELAS = ARBOL DE DECISION, del cual se podría dibujar el árbol: mira la partición "MÁS GRANDE" y luego las sucesivas más pequeñas.
		OVERFITTING, A MÁS DEPTH MEJOR PERO NO GENERALIZA BINE PORQU EHEMMEMORIZADO TODO. --> GRID SEAARCH PARA VER LA MEJOR PROFUNDIDAD
		

- EJERCICIO (TITANIC):
		DISCRETIZACIÓN DE VARIABLES (edad): DEBE DE SER LÓGICA

- KNN: MODELO lazy (not eager), BASADO EN DISTANCIAS 
		VOY INCREMENTADNO K (1,2,3,4,...) Y SE VA COMPARANDO CON OTROS MODELOS.
		+ k = + generalidad y fronteras más rectas (pero con k más baja (1-9), suele dar valores correctos también)
		 EJEMPLOS MEZCLADOS Y MUY PEGADOS A LA FRONTERA DE DECISIÓN SON MUY MUY DIFÍCIL DE PREDECIR (INSTANT HARNESS, PSICOLGY, ...)

- SVM ALGTH (support vector machine): BUSCA LA MEJOR FRONTERA LINEAL (MEJOR HIPERPLANO, EN MUCHAS DIMENSIONES) CON UN MARGEN MÁXIMO CON VECTORES DE SOPORTE. SI LOS DATOS NO SON SEPARABLES --> KERNEL TRICK. 

EJERCICIO: 
		HACEMOS DROP DE LAS COLUMNAS PERO LUEGO LAS UTILIZAREMOS PARA MEJORAR EL ACCURACY


-------------------------------------
14/10/25
-------------------------------------
- ENSEMBLE ALGORITHMS: combinación de algoritmos
		HISTORIA: Wisdom of the crowds (i.e. mean of everyone's knowledge about cows weight).
		SINGLE: 1 model
		VOTING: 
		BAGGING: random forest (choose number of trees + try to have DIFFERENT TREES (diif information))
		BOOSTING: XGBoost, next model tries to solve earliers error, same with the following, and so on.

- VOTING: 1 training set, SEVERAL DIFF CLASSIFICATION MODELS at the same time, hard/soft voting (hard: majority votes, soft: average probability class) in the example we can see how 0s have a lot more confidence than to 1s. Soft vote is more recommended. 

BEst models at final Project end up being a voting ensamble ->
		-> alpha*RF+beta*XGBoost+Thet*LAsso=1

- BAGGING (Bootstrap AGGregatING): they use mostly decisión trees (problema that with the same data, they Will always give the same decisión tree, so we need them to be INDEPENDENT and not to CORRELTATE BETWEEN THEM)  -> DIVERSITY. (ALEATORIEDAD FUNCIONA MEJOR)
	-> BOOTSTRAP SAMPLING: muestreo con repetición del mismo tamaño que el inciial (algunos elementos están repetidos, para tener el mismo tamaño que el inicial). ESTÁ DEMOSTRADO EMPÍRICAMENTE QUE A MÁS ÁRBOLES, MEJOR RESULTADO.
	-> recoradmos que son muy dependientes deldataset de entrenamiento, con lo que hay mas varibailidad (los que más se repiten tienen más relevancia --> APRENDIZAJE BASADO EN COSTES)
	-> FEATURE RANDOMNESS: …
	-> PERMITE PARALELIZAR MUCHOOOOO (son independientes)
	-> SON MODELOS QUE GANAN MUCHAS COMPETIICONES

¿¿¿manipular el sampling para buscar algún tipo de diversidad/ aleatoriedad concreta se hace? Este caso no explora todas las posibilidadesdel problema? ???

- BOOSTING: en cascada, van comprobando los errores del anterior.
	-> Assigns weights to errors in every SEQUNECE (THEY ARE sequential).
	-> ADA BOOST: increases inaccurately classified data automatically (such us in DT) in each iteration and at the end combines every weight.
	-> GRADIENT BOOSTING: tries to predict erros (minimize previous model errors). XGBOOSTING: mpstly used, not in scikit-learn, very costly (because of stable/balanced tres with not too deep branches, which are the best, but manipulating them is costly). LIGTHGBM are less costly



-----------------
Unsupervised: grouping and RULES
	-> Grouping: number of groups is hyperparameter (decided from DENDOGRAM)
- HIERARCHICAL:
	-> AGG-CLUSTERING: BTM->UP, first iteration: instances are cluster, second: closest ones are cluster-> closest clusters e cluster.
	-> DIVISIE CLUSTERING: TOP->DOWN
	-> WE DECIDE WHEN TO "CUT" THE CLUSTER
	-> DISTANCES: DIFFERENCES OF VECTORS (INTER-CLUSTER DISTANCES), WHO DEPENDE PN THE DISTIRBUTION OF THE DATA (WE CREATE ALL THE DISTANCES AND DECIDE ANNALLYTICALLY)

- NON-HIERARCHICHAL:


--------------------------------------------------
16/10/25 PROJECTO ML
--------------------------------------------------

- Overall QUality --> num vs. discrete
- Change to numerical
- NA detected as NaN: but for a basement it is not, jsut has no basement. CAREFUL
- DATA EXPLORATION: important
- No nº of baths but nº of baths in every floor: créate a new one.
- ¿? Look for refrences in housing webs, to see what people look for ¿?
- 80 variables input y 1 output (1500x81)
- Apply changes to train also to test (81 var at train, 80 at test), if f(x_1) to train, aslo to test. (COLUMNS ESPECIALLY, ROWS ONLY IN TEST)
- Check Code and Discussions
- fmartinez
- Menos variables y más lanzar modelos, muchos
- Lasso, Ridge, (pues es normal tener overfitting)



-------------------------------------------------
28/10/25
-------------------------------------------------
NLP: Comunicación Hombre-Máquina a través del Lenguaje. 
Lingüística (normas), Lingüística Computacional (intenta aplicar normas, sistemas interpretan reglas)
Part-of-speech: asignar categoría a cada palabra (ej: banco es un sustantivo), tarea sencilla con 97% d eprestaciones, pág 8 intro.
Niveles de análisis:
Problema de la ambigüedad: 

regex101.com


-------------------------------------------------
03/11/25
-------------------------------------------------
Integer encoding: para SIMPLIFICAR EL TEXTO antes de trabajar con él.
OOV: Out Of Vocabulary (to tf.Tokenizer, has to be explicitely added)

Rpresentation model:
1º: Pre-processing, deleting stop-words
2º: Appears -> Frequency -> 
3º: Document-Term Matriz (DTM) -> Term-Document Matrix (TDM), which have the same content. + BOW
	Bigger document will favuor certain words frequency with the possibility of them not being as important
4º: Normalize the frequency of the Word (we divide by the length of the document)-> IDF
5ª: TFxIDF Matrix (if we use zeros at IDF it becomes too radical, so we try to avod it).

Embedding: vectorial representation of words.


-------------------------------------------------
06/11/25 Deep Learning: Carlos Monserrat
-------------------------------------------------
- Neuron: Input -> Processing -> Output to other neurons (1 billion connexions the avg. brain, GPT-5 1,5 billion).
- McCoullugh + Pitts: 1, 0, -1: integrate if over a x: output yes. PERCEPTRON
- Rosenblatt: values are real and if over x, send signal.
- Neuron (Rosneblatt): z = sum(w_i * input_i) (1st part), 2nd part: umbral con 0-1 output  ---> it's a regression, but this nueron is limiteed to goign though axis, bc has no independent term. If we add it -> we get all possible lines, REPRESENTED by adding an input with fixed vlaued of 1 * coefficient, resulting in: sum(w_i * input_i) + b. 
   NEURON OUTPUT HAS SUM AND CONDITION (activation function).
- Misky Paperrt: proved is too simple and cannot even solve XOR function (we cannot draw a line that seprates 0,0 and 1,1, from 0,1 and 1,0 if you view them as points on a plane, pag 17), 1st: "WINTER", as no one was interested -> Minsky proposed a several layer but difficult to calculate weigths.
- PERCEPTRON has 2 capas: Inputs and output (neuron), x_0  is for independent term equlled to 0.
  - For every input we calculate a Z_i and make it to 0 or 1 if positive or negative (ex: z = w_1*x_1 + w_2*x_2 + b) X_0 IS ALWASY 1 (b, independetn term).
- How to calculate weights fir what we want as a result.

- Improvemetns after multilayer and backpropagation techniques, but PROBLEM OF WEIGHT CALCULATION.
- P. Werbos: proved that given an error, we can calculate a goal function thorugh cprrection of those weights BACKPROPAGATION. (We don't know f(x) but we know a f^^(x) so we start with that and we start to "fit it" to our goal function through error correction BACKPROPAGATION).
- Rumelhart proposed applicaion of backpropagation to multilayer perceptron,

- 1st layer "TRANSFORM THE POINTS" nad the second neuron applies the "cut" regression. (such as modifying the plane to "find a plane transformation" and then actually make the regression to that).
  - A HIDDEN LAYER must be added (over 4 hidden layers it's called DEEP LEARNING, overfitting and vanishing, same problems as 50-60 layers).
  - How many parameters have to be instanced to hidden layer (coefficient): 6 for input, 3 for output.

Exercise:
N1 = OR  (1 = 0,1/1,0/1,1 /// 0 = 0,0)
N2 = NAND (1 = 0,0/0,1/1,0 /// 0 = 1,1)
N3 = AND (combines n1 and n2 to create XOR)

- Backprogatation needs a continuous function (SIGMOID IT IS, NOT THE STEP FUNCTION (threshold function), it is also INTERPRETABLE AS A PROBABILITY FROM 0 to 1).

MLP: INPUT LAYER + n * HIDDEN LAYERS + OUTPUT LAYER
PARAMETERS FOR A 5 LAYER: ((12 +2) + (16 + 4) + (16 + 4) + (4 +1)) = 


Recommended books (each book has a GitHub page with exercises to use):
- Hands-on Machine Learning with scikit-learn, keras and tensorflow (O'Reilly) BASIC 
- AI and Machine Learning for Coders, O'Reilly (about Neural Networks) INTERESTIC AND DIVES DEEPER 
- Building LLMs for Production, O'REILLY, by L.-F. Bouchard (NOT BAD, COULD BE BETTER), 2024
- Building Applications with AI Agents, O'REILLY (GREAT FRO CREATION AND PRODUCTION, ...), Oct-2025, shows DANGERS of this type of PROGRAMMING.
- Model Context Protocolo, Mehul Gupta, et.al.., O'REILLY, , (AI AGENT CONNECTS WITH TOOLS TO CONTEXT AND GENERATES OUTPUT, doe snot need to train the model with PowerPoint, Chrome for web-scrapping, ..., DANGER OF VENONOUS FUNCTIONS.), July 2025.

Exercises: (pág 34-37)
For a 4->3->4->1 neural network, for a x^1_i neural network, we have a [[[Activation_func(w_ijMATRIX * [x_i] + b_i)]]], for the FIRST HIDDEN LAYER. SECOND HIDDEN LAYER, THIRD.
Activation funciton can change between layers, NOT BETWEEN NEURONS OF THE SAME LAYER.
At the end we have a value y^^. WE have ERROR = y - y^^ = LOSS FUNCTION. We update weights and make the neuron "learn" and adjust better.
BACKPROPAGATION: WE HAVE TO "DIVIDE THE ERROR BETWEEN THE COMPONNETS (MULTIPLY BY "LEARNING RATE"), we use DERIVATIVES TO CALCULATE HOW MUCH INCREASES AND DECREASES (problme is that we may find over and underfitting -> view a REAL loss function, with local minimums, ...).

------------- FOR NORMAL NEURAL NETWORKS, NOT DEEP ------------------------
Hyperparameters: 
	hiddenlayer number (the more complex, the more layer, but try not to go over 4, so 1,2 or 3... MORE IS RARE),
	number of neurons per layer (INCREMENT TO ALMOST DOUBLE the number of neurons at the start,
	first to learn big and then reduce to smaller.),
	loss_function (Error=),
	optimization (function to optomize, such us gradient descent or ADAM -> WORKS REALLY GOOD),
	learning rate (too much may provoque oscillation, too small can make it trapped in a local mínimum -> ANNOYING TO FIND),
	epoch number (AFTER ALL SAMPLES HAVE PASSED THROUGH LAYER, THAT'S ONE EPOCH, stop at epoch with the best fit, in case it overfittd, or it stops at a certain type, so we stop it --> CHECK THAT IT CONVERGES),
	batch size (to decide who big is gonna be the backpropagation, but if bathces errors is big, it learns slwoly --> IF MORE OSCILLATIONS BIGGER, IF DOES NOT CONVERGE, REDUCE).
- Starting number of neurons depends on the classes.

VERY IMPORTANT TO DRAW THE CURVES OF VALIDATION AND TRAINING  (X-AIXIS: EPOCH, Y-AXIS:(MINIMUX, LOSS FUNCTION)): DRAW TRAINING AND VALIDATION.
   - FROM TOP TO BTM, BOTH TOGETHER: IDEAL SITUATION.
   - Validation top diversion whilst traiin redueced: reduce nuerons or layer bc it overfitted
   - If both don't go donw: increment hidden layers or neurons bc it DIDN'T LEANR ENOUGH.
   - Converge great till a point but then validation diverges: overfitted after X EPOCH, so STOP AT GIVEN EPOCH.
   - RARE BEHAVIOURS: look like it's not learning and then it learn a lot after X EPOCH.

HYPERPAPRAMETERS TO REDUCE NETWORK "EXPLOSION": (L1, L2 // also BATCHNORMALIZATION), L!: it's th eabs value between the error and the othe it's squared, so we garantee that the network does not get crazy ---> NAN PREDICTION MEANS OUT OF RANGE 
HYPERPAPRAMETERS: Dropout, when overfitting + ruido...


FUNCTIONS: 
- Threshold is NEVER USED, TOO OLD.
- SIGMOID: advantages but too "SOFT" (used only on output layer to get PROBABILITY INTERPRETATION), use SOFTMAX TO calculates SIGMOID AND NORMALIZES SO SUM OF ALL OUTPUTS IS 1. NEVER NEVER USE SOFTMAX IN INTERMEDIUM LAYERS NEVER NEVER.
- TANH: BIGGER CONVERGENCE BUT LESSER INTERPRETATION, not good for classification output.
- Linear: does nothing, like a NO ACTIVATION FUNCTION. IT'S A REGRESSION, GET'S THE NUMBER, PREDICTS THE NUMBER, ONLY INTERESTING ON OUTPUT LAYER (equations get a lineal combination so matricially is like nothing happened).
- ReLU: (only positive side of linear or 0), so no lineal combination can happen. GREAT FOR DEEP LEARNING, GOOD RESULTS, WITH EASY CALCULATION, ONLY FOR HIDDEN LAYERS, LEhyReLu, does not truncate to 0, but multiplies negative value with Alpha and receive a value code, NEVER ON OUTPUT, great to NO LOOSE INFORMATION. 

--------------------------------------
07/11/25
--------------------------------------
Deep Learning problems:

a) Signal propagation
- Propagations: big oscillations and "explosions" -> reduce learning rate:
- Vanishing (does not learn): takes many epochs to learn and sometimes maybe it does never learn. When "dividing the error between layers, IF WE DON'T GET TO 1ST LAYER, DOES NEVER LEARN. ---> Use activsation function that share error properly (sigmoid has difficulties with that, it is too soft (1sr derivative is too soft), better with a tanh and if it is deepere, use a ReLU)
b) Initialization:
- Weight initialization: must be random, cannot be all the same because they all would learn the same. It is demonstrated that following the formula of pag.50., paper (http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) 
c) Overfitting:
- Regularization of weights: with L1 and L2., similar to Lasso and Ridge (they try to avoid a too big increase of th error to not overestimate the importance, better use small values).
- Dropout: exclude nodes to avoid over dependency (they are overtrained).
- Data augmentation: add noise, create data by transformation (implemented on libs) (use when it doe snot learn, does not generalize well).

DROPOUT: avoid overtraining (in tensorflow game, they are not heavily colored or too much colored) ---> BETTER IF THEY ALL CONTRIBUTE SIMILARY, DO NOT ESPECIALIZE. Dropout randomly disconnects neurons by layer over a batch adn back propagation (only activated neurons).If a neurons whas specialized, it will stop it so other neurons can "catch up". 
 AFTER TRAINING IS DONE, WHEN DOING REAL PREDICTIONS, ALL NEURONS ARE ACTIVATED, SO DROPOUT ONLY OVER TRAINING PERIOD.

----------------------------------------------------------------

TENSORFLOW: works only over Linux, supervised by Google, works over CUDA- GPU, libraries to .js and Lite allow to work on Google Chrome and mini-computer. TRAINING IN BIG COMPUTERS AND FINAL NETWROK wherever you want.
KERAS: ...¿?

TENSOR[rank]: scalar[0], vector[1], matrix[2], n-d matrix [3] with info SUCH AS DATATYPE, DIMENSIONS... (np.array-like object), 

----------------------------------------------------------------

For a 28 x 28 pictures: 
NEURAL NETWORK inout: has to be a vector (given a nxn matrix, trnasform in a n*(n/x)*x). 

Neurons
Input layer: neruons (784)
Hidden layer: nuerons (100)
Output layer: nurons, as many as classes we have (10)

Params:
Paramas of input-hidden: 78400 + 100
params of hidden-output: 1000 + 10

-----------------------------------------------------------------
CPU: parallel and independent
GPU: parallel and depedendent, all with the same problem, so perfect for neural networks as thye have the same problem over and over (calculations).
To use GPU in computer yo need CUDA to apply tensorflow (can tell you waht resources are available).
Specific GPU needs a specific versión of CUDa which nees a specific versión of Tensorflow that HAS TO RUN OVER LINUX. 
WLS: we can install a Linux over it.
-----------------------------------------------------------------
- Autocorrelations are problematic for neural netqork models, so try not to have the correlation of one hot same as the number of clases (for a 3 class, instead of 3, have n -1 = 2), until the end output has to be 3 classes (as many positions in vector as classes).
- Change of datatype can fuck upo the neural network, so beware of it and use casting functions.
- It is interesting to create randomized normal distirbutions for Generative Neural Networks ¿?¿?¿?
- With image processing, sometimes you have to remove voided dimesnsions, such as the 1st dimenisons. as shown in th eexample (REDUCES DIMENISONALITY)

----------------------------------------------

-------------------------------------------------------------------
10/11/25
-------------------------------------------------------------------
- Keras: library for better construction of NN.
- Keras exercise 1: logistic regression, with gradiente descent and Mean Square Error

- Best to use a NN that already exists (as some work better than others, so easier to get an old one adn refine it rather than creating one from zero)
- Always do TEST, but better: train -> validation -> test

- Keras exercise 2: make another NN but using the sigmoid function as activation function. Error would be the different of the neuron and compared to the Y, they are PROBAILITIES, as the sigmoid funciton returns a number from 0 to 1 that IS a probability.
		- As it is a classification and no regression , loss fucnction can only be binary_crossentropy or crossentropy
		- For a classification, better to add another paramtere metrics.

Errors formulas, pág. 115:
- MSE -> better for bigger errors than MAE (which could bring more stability). No predictions are decisive
- BCE -> 2 clases -> CCE, several clases.
Parameters and funcitons: (pág 116) -> check (spares_Categorical_Entropy and Multi-Class Classification PRoblems, beware of categorical one-hot encoding, must be done with 1st instruction, 2nd does it automatically, BEWARE of data leakage.)

Backpropagation: steps on the mountain towars a valley metaphor.
BAtches: to prevent that outliers fuck up and make the NN unlearn, we define batches, so let's say we make a subset of epochs. For 10 epochs, 100 bacthces, calculate several errors, maks the average and then we clacualte verything and apply it with functions, so backpropagation does not fuck up. It is expensive computationally, makes noise and outliers dissolve in th ebig chucnk of study,. When using GPU, passing from MP to GPU it is faster to send seveeral sets to GPU at the same time, EVEN BETTER TO send a BATCH, and whilst it is working, be preparing the following one. BAtch managing can make the NN improve better than the funciton sometimes, so it is VERY IMPORTANT.
AFTER TRAIN TEST THE MODEL, ALWAYS!!! + LEARNING CURVE COMPARING TRAIN AND VALIDATION PER EPOCHS (THEN DO THE TEST).


Error propagation functions (optimizar parameters, pág. 122):
- Gradient descent: local min problematic, 
- Momentum: jump but also length of the step (wioch depends on the previous step)
- Nesterov Momentum: similar, but if next jump is positive (movement on the Good way, but if negativ, reduce as proven that it is not the Good direction, so freezes that parameter). Recalculates jump over previous jumps if oscillations detected.
- Adagrad: decide a step and reduces it over time (the more updated whas the gradient, the smaller the step)
- RMSProp: similar to Adagrad, but only takes some epochs, not all the historic like adagrad (similar to how the spiral tensorflow playground evolved, slow, then fast, then slow again, the fast,... won't be posible with a normal adagrad).
- Adam:  best propoerties of Momentum and RMSProp
- RMSProp and Adam are the most used by proffessor.


- Keras exercise 3 (NN that checks if student passes or fails):
	- 1 neuron as input
	- 4 neurons as hidden layer
	- 1 neyrons as output
	Parameters are: (1*4 + 1*4) + (4*1 + 1) = 13 (conexiones + sesgos de neurona (extra parameter per neuron))

--> (Bonus): Piramidal layer structure: neurons can receive more info and then summarize (so better to start with a lot and the reduce). With convonutional you can see live: from start only lines, in end you can see faces and from output to inside, you can start to see faces, eyes, etc... (so at the end you need lesser neurons)
--> 1db: to check evolution of your training online.

TENSORFLOW FUCNTIONS: …

DL DISTRIBUTION: Keras > TensorFLow > CUDA > GPUs         (GPUs same program, CPU, different independent programs)
                 (APIs)   (FRamework)  (Math)  (HW)

- Keras exercise 4 (NN that checks if student passes or fails):
	- 1 neuron as input
	- 4 neurons as hidden layer 1
	- 4 neurons as hidden layer 2
	- 1 neuron as output
	Parameters are: (1*4 + 1*4) + (4*1 + 1) = 13


LAYERS to CREATE: pág 144
	- Convd1: sound, text
	- Convd2: images
	- Dorpotut: prevent oscillations
	- FLatten: don't learn
	...

Common DATABASES: … pág 145


--------------------------------------------------------------
13/11/2025 CNN
--------------------------------------------------------------
Classification with CNN + NN
- Input data
- Feature extraction WITH CNN
- Classification with a normal NN
- Output


- Channels must be added and images need to be normalized (NN behave better with normalized samples).
- Usage of categorical_crossemtropy (need to do One-hot encoding) if using Sparse_categorical_crossentropy, NO NEED.

- Add Dropour Layers to prevent neurons from SPECIALIZING ON WHATEVER.

WE LAENR A LOT OF DETAILS(CONV2D), THEN WE HAVE TO CHOOSE THE BEST OF THOSE (MAXPOOLING), THEN WE HAVE TO (FLATTEN), THEN WE HAVE TO LEARN WHAT OF THOSE ARE DETAILS THAT NEED TO BE LEARN (DENSE), WE NEED TO DROPUT THE DETAILS THAT ARE NOT IMPORTANT (DROPOUT)


EXERCISE 1 (CNN)
- ·2 kernels, the una imagen, crea 32, y las apila en la dimension finalcada una generada pr un calnal differente), pading='same', es por el problema de los filtros (en el borde de la iamgen, el tr trozo que se sale de la imagen ---> ¿? COn el 'same', haces que aunque se salga del kernel, se tenga el mismo tamaño de imagen: se hace 0, si no es 'same' y es 'valid', se olvida de esos oixeles, por la imagen es más pequeña, por lo que tendrá que dejar caer algunos pixeles para tener una imagen del mismo tamaño): CASI SIEMPRE S EPONE SAME, EXCEPTO APRA LSO CASOS ESPECIFICOS. 
- Se pone MaxPulling, para reducri lso detalles, pues no necesitas tantos: REDUCE EL TAMAÑO DE LA IMAGEN. From that 2x2 defined dimensión, takes only 1 pixel, so redcues size to half.
- AFTER A CONV2D WE MAKE MOST OF THE TIMES A MAXPOOLING TO GRAB THE DETAILS THAT WE WANT
- After this: we want to FLATTER, make it a 1d vector: converts to input neurons, so it reduces the dimensions
- WE have to use DRopout to prevent overfitting, BECASUE TO MANY PARAMETERS MAKES MORE COMPLCIATED FUNCTIONS THAT OVERFITT THE MODELS



RECURRENT NEURAL NETWORKS:
- Inputs + Outputs to itself and to other neurons of the SAME LAYER. They have MEMORY (they can represent temporal sequences.).
- Sequence-in and Sequence-out representation: development through time. Through time we have output every time but we are interested in th elast output.
- Encoder: has inputs and evolves through time (we ignore output).
- Decoder: we have no input, just output.
- BACKPROPAGATION: only to the ITERATIONS THST WE WANT. WE prograte backwars to previous iteration, update paramterters, then we update again, them we update paramters and so and so.
- FORGWTTIN PROBLEM: first wprds in backproagation are ok, but th emore backwards we go the more it "forgets".
- LSTM: Long Short Term Memory: saves in another channel memory for the long time 
- Beacus eof the characteristics of these NN, after 2 layers they can already be thougth  as DL, because the NUMBER OF PARAMETERS GROWS EXPONENTIALLY.

EXERCISE 1 (notes):
	- Scaling reshape: Batch number, samples and atributtes (price of shampoo in this example)
	- Attention to resumen:fit()


---------------------------------------------------------
17/11/2025 WANDB
---------------------------------------------------------
Experiment tracking: is whatever training used to find which one is the best model to use (an experiment is whenever you change parameters of the model, but don't chnage hte code): ---> control htrough "wandb" (GitHub looking with dashboards, online tracking, saving models while training, watch model though training when not at home (remote tracking)), "TensorBoard".  

WANDB: 	- login()
	- init(project, name, config)
	- It is recommended that the filename gets a value in th name, so that you have some info in the name
	- To extract config values: assign to a variable.
	- Use a callback var to sabe the best model
	- BEWARE of FINISHING THE CONNECTION with Wandb
	- BEWARE OF NOT OVERWRITTING THE SAVED MODEL
	- Saved model can be the last one, previous to last, … (modify ModelCheckpoint())
	- All METADATA defined in th eproject areimportatnn so that OTHER PEOPLE CAN REPRODUCE THE EXPERIMENT INDEPENDENTLY. Also, such as GitHub, even if you lose the model, if most of th einfo is defined there, it could be easily reproduceable next time.	

¡¡¡¡ Every time, OPEN, CHANGE NAME OF EXPERIMENT and CLOSE !!!!


TENSORBOARD: 
similar but inside COLAB's FILE STRUCTURE. 

-----------------------------------------------------------
CNN:
	- Yann LeCun's proposal (1990), not good computing power yet.
	- AlexNet: application of GPU to NN
	- CNN since 2014 have been the strongest 
	- Imitation of primate brain focus (separated and overlapped focus)

- BETTER USED KNOWN STRUCTURES THAT ALREADY WORK FOR A CNN (look in HugginFace for example to see the best structures).
- Convolution filter: the productorio of all weights and pixels of a región so that we obtain a pixel with the value of around pixels and also overlapped. Given by the function: O(,) = sum(sum(K(x, y)*I(i+x, j+y))) which is applied around the image. WE can process q pixel per image entry (so the calculation Surface moves 1 pixel) or several. If we do a desplacement of 2, the resulting image is half the size (normally we don't do that, just small steps). 
- Padding: with a 1 step displacement, when we get to the border pixels, kernel is put of th image. If they are not processed, resulting image is smaller ('valid'), the other option is padding with zeroes ('same').
- WE can draw a kernel to "look for" certain shapes: in the places where the shapes match, the resulting number is bigger ("brighter"). IF we apply a threshold, we can get back only the shapes wanted. WE can detect structures, shapes, frecuencies and SPATIAL KNOWLEDGE (position in image) ... THE CNN learns what is the best kernel to find the characteristics you want. (SO THEY LEARN CHARACTERISTICS).
YOLO: object detection thourh learn characteristics through CNN
- KERNEL SIZE: normally 3x3, up to 7x7, too big means too GENERAL. Can be squared or not and have a n=even or odd, but odd from a coneceptual POV is better.

IN A TOTALLY CONNECTED (NN) OF 28X28 IMAGE, PARAMS ARE 28X28=784+1= 785 PARAMETERS. IN CNN, INPUT IS (28,28) --> for a 3x3 kernel -->9+1=10. SMALL PARAMETERS, BUT STILL APPLICATION OF KERNEL TO ALL PARAMETERS.

- When looking at a CNN draqwed, inputs and outputs are drawn, but not all the neurons. A 3D box is usaully drawn, saying Conv(nxn)x1024, weach means it has created 1024 images from a 5x5 kernel. If several layers, a depth is added to the maximum layer needed (THIRD DIMENSION IS NOT MENTIONED BUT IS THERE), so a mxnx3 image will have a 3rd dimension on the resulting images.
- Kernel number (1024 previously): always given but a multiple of 2, because of computer management.

Given that imprirically:
- 1st layers: learn details (feature extraction): (Convol. + pooling)*n
- Lasts layers: learn generalizations (classification): (Dropout layer + fully connected layer)
So: PYRAMIDAL SHAPE:
- DEtails first: so we learn a lot from all this things
- Generalization layers: when I have many learnt and with good knowledge things I can reduce the images (MAXPOOLING)
AS YOU DON'T KNOE WHAT IS THAT THE KERNEL WILL BE FOCUSED ON: THE MORE THE KERNELS, THE MORE IT WILL LEARN, THAT IS WHY THERE ARE SOM MANY LAYERS, BUT TOO MANY WILL OVERFIT, that is why PREDETERMINED STRUCTURES, AS SAID EARLIER, WORK REALLY WELL!!!

MaxPooling and AVgPooling: let you FOCUS ON THE DETAILS, so that we can DROP useless info. THWY DON'T LEARN. They take a block, choose the highest value and drops the rest, so in a 2x2, we drop it half the image. AVG: does the mean, rather than just the maximun. BUT MAXIUM IS THE MOST USED, BECAUSE THE MAXIMUMS ARE MORE IMPORTANT THAN THE AVERAGE, BECAUSE THE MEAN DILUTES THIS "DETAILS".

Example to manually calculate: pág 38 (no padding -> smaller img), 39 (0-padding).
- Images usually replicate images on the padding instead of zeroes, because of grey colors, which creaet frecuency noise (big changes on grey generate big frecuency changes).


------------------------------------------------
10/11/2025
------------------------------------------------

Every Kernel Works like as a neuron, so aprameters are calculated as nxnxn. [(width x height x depth + bias) x neurons] x same_type_layers.


- What is the best Depth to work with: depends on image size, things to learn, …
- Multilayer is recommended to build in 2^x because of computing reasons (from 8 neurons to 16, from 16 to 32).

- Usual tips: use the biggest posible WORKING network to see if it Works in training and then apply rules to prevent overfitting, such as REGULARIZATION (L1 and L2).
- Professor says that to him it almost never Works to better result, just uses them when network fails (i.e., when all neurons show NaNs).
- What profesor says better works fo him: learning rate, batching, DROPOUT, neurons and layers, almost never l2.
- Batch normalization, l1 and l2, only when NETWORK FAILS, that's what has worked with profesor, regarding personal use. Uses more l2 than l1.
- Also deleting layersand neurons to prevenet overfitting Works.

- Regularization is added in the hidden layers, a big coeffciient makes that there is no "disparo", so 
network does not learn.

- (Categorical/Binary) cross-entropy: WHAT TO USE? LOGIT (BEFORE APPLYING THE LOSS FUNCTION) or NOT?
	Logits is more stable than probabilities, so when calculating the loss function they add a calculation that solves the msitakes of the computer, run calculations a bit slower but they provide stability to the network. IN TRAINING, THEY INTERNALLY DON'T EVEN USE THE PROBABILITIES.


NETWORK:
1st layer (Conv):
	Neurons: 32, kernel of 5x5 and 32 kernels, so 32 neurons.
	For every kernel, an image
	Params: 5x5x32 + 32 = 832 parms.

2nd later (Pooling): of 2x2, so reduces images to half size.

3rd layer (Conv):
	Neurons: 64, kernel of 5x5x32 (BC IT RUNS THROUGH ALL INPUT IMAGES) and 64 kernels.
	Params: 5x5x32x64 + 64 = 51_264

4th layer (Pooling): of 2x2 so reduces image's size to half. 

5th layer (Flatten): converts to flat neurons, a straight line
	Neurons: 7x7x64 = 3136 neurons.

6th layer (Normal NN):
	Params: 3164x1024 + 1024 = 3_212_288

7th layer (Normal NN): 
	Params: 1024x10 + 10 = 10250


- RNN: prediction is based not just input but also previous input!!!

RNN layer: 7 inputs for 3 RNeurons

weights = (7 + 3) * 3 + 3 = 33

Problems that RNN have: that when correctign th weights, they all deopend from the previous one and expanfing backwards which produces a diluting effect on previous predictions (LATEST PREDICTIONS HAVE MUCH MORE WEIGTH OVER EARLIEST PREDICTIONS WHICH END UP VANISHING (totally after 10-15 neuron)).

LSTM (Long-Short Term Memory): in pág. 96 there is a draw which represnet it. Every blue box's output has the number of neurons (all of them the same number), so previous predictions has also size 4. 
	Y_t-1: short term memory
	C_t-1: long term memory (important thing), also input of size 4
	PUERTA DE RECHAZO: Frist big X is the multiplication function that DECIDES WETHER SOMETHING IS IMPORTANT OR NOT.
	X_t: current prediction
	PUERTA DE RECUERDO: decides what should be remmbered so it is added to C. Uses a sigmid bc of the 0-1 probability and the tanh bc of the postiive, negaative or 0 values.  
	Last sigma: combines itself With the lomng term memory (actually, the long term memory gets tanh_ed? and then combined to the current predcition and giving the Y_t as current output and C_t as memory)
	NEURONS ARE ONLY ON THE BLUE DOORS.
	
	C_t would GO BACK TO C_T-1 and Y_t to Y_t-1.
	For a RNeuron with 5 values at X_t, we would have as parameters:
		X_t: 5
		Y_t-1: 4
		Inputs: (5 + 4)x4 + 1*                            *(NEURONS SHARE THE BIAS, SO ONLY 1 ACTUALLY)
		4 DOORS: [(5 + 4)x4 + 1*]x4



-----------------------------------------------------------
20/11/2025: RNN
-----------------------------------------------------------
Tomorrow:
- API key Mistral (free)
- API key Chatgpt (free for x tokens)

Yu can find networks on:
- keras.io/api/applications


Making a RNN that "speaks like shakespeare".
	- Input layer: size 39 (one letter)
	- GRU layer: 128 neurons
	- GRU layer: again 128 neurons 
	- Dense layer: you can choose wether or not the Dense receives information until the RECURRENCY IS DONE. If set to false, does not send info, if TRUE, send info everytime, every recursión. We would use the False if we wanted to classify a text, i.e., where we don't need instant result, but a final result.


----------------------------------------------------------
21/11/2025: Generative Adversarial Network (GAN)
----------------------------------------------------------

ID organización Mistral:
358d7b40-647e-4764-a722-ac3a62181259
API Key:
FwppSWxoFXbkwro9Wg1JCzGVxnK4uxJj

BASE OF THEM IS AUTOENCODER

GAN: créate something from an unidentified input. From pág 111, encoder gets all the faces, decoder gets a seed number and we get all this AI generated faced (from an input-value).

As I do not know hte distribution of values, if I input a vector that is not INSIDE THE LATENT SPACE, not in the distirbution wanted, Will NOT GENERATE ANYTHING INTERESTING. We must transform it into a normal distribution, for example.

- Variational Auto-Encoder (VAE): this variation of autoencoder forces the input (X^_) to be "normalized". For every class, we Will have an avergae, and standard deviation from every class and add it to a database, so that we can generate a value inside that DISTRIBUTION.
- Interpolation of two values: taking an avocado and a sofa, generated from a random vector inside dsitribution, with INTERPOLATION, Will generate a sofa with avocado shape, for example.

- Generator / Discriminator:
	- GEnerator: creates an image from a random vector.
	- Discriminator: from a real image, decides wether it is a real or fake image.
	NONE CAN BE BETTER THAN THE OTHER, so Generator has to be of similar PERFORMANCE to discriminator, because if it is better, it Will not learn. THe opposite can happen. THA TIS WAY THEY USAULLY HAVE SIMILAR STRUCTURES.

- 1st pase: Discriminator is trained whilst Generator is "frozen", using back propagation to differentiate both images (TRIAN DISCRIMINATOR).
- 2nd pase: Discriminator is frozen ang Generator learns (TRAIN GEN ERATOR). 












		






	