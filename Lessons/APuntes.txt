Dim = 2

Cada movimiento de matrices tiene un movimiento en ele plano asiciado y vicecversa.
Cuando haceos f(vector) = A*vector. Esto es, cualquier función / onivimeinto que hagamos tien euna matriz asociada.
Por ejemplo:(3 0) * A = (0 3)

Simetría:    (0 1)
             (1 0)
Giro de 90º: (0 -1)
             (1  0)
Matriz invriante: M * v = lambda * v (domde v es el 'vector propio', lambda es el valor propio) (autovalor o autovector)
vector proio = son los 'direcciones invariantes'
lambda contenido en R es 'el multiplo' positov o negativo 


FACTORIZACIÓN:
Si M que es parte E de una M (NxN) de Reales.  tiene n vecotres propios linealmente indeoendientes, se cumple que M = P^-1 * D * P, donde D = matriz diagonal , 
LOs vectore spropios independientes se pueden obtener como resultado de A-lambda * I = 0 (de sus ecuaciones derivadas).
Los vectores linealmente independientes s eponen en vertical en P y corresponden con cada lugar de D = v11 está en (1, 1) y corresponde con la columna 1 de P.

M^2000 = costoso.   P^-1 * D * P = D ---> M = P * D * P^-1, con lo que ----> M^2000 = P * D^2000 * P^-1, puesto que poneniendo: P * D * P^-1 * P * D * P * P^-1...., donde P*P^-1 = I

- El procductorio de otdos lo valores de lambda da el determiannte de M.
- El sumatorio de todos los valores de lambda (suma de los autovalores) corresponde con el sumatorio de los valores de la diagonal de la M inicial. 
- Si M es simétrica (), todos valores propios (lambda), son números Reales.

Con una matrix M mxn, se puede trabajr con M_Especial = M^t(mxn) * M(nxm), para obtener una matriz cuadrada.
M(mxm) = (mxn)x(nxm)  -> lambda de 1 a m
M(nxn) = (nxm)x(mxn)  -> mu de 1 a n
Una de las matrices tendrá más 0s que la otra.

- En A^t * A === A * A^t tienen  los mimso valores propios en uno qu en otro.
- Todos los valores propios no nulos son positivos (puesto que se han multiplciado esos valores y por tanto NO PUEDNE SER NEGATIVOS) y se pueden ordenadr de menor a mayor
Valores singulrares de A = son las raíces cuadradas de todas las lambda (y ESOS son los valores, ya que el restultado de la OPERACION DE A(mxn) * A(nxm) *duplcia* valores. 

Las matrices U y V son (nxn, A * A^t) y (mxm, A*t * A) y multiplican a *E* (sigma), por la dcha e izqda: U * *E* * V. Sigma será una matriz diagonal, la U será una matriz horizontal y V será una matriz vertical, de tal manera que obtendremos un SUMATORIO DEL PRODUCTO DE TODOS LOS VALORES DE sigma * u * v, para tods los valores 'i' por los que se extienda. U y V son ORTOGONALES, puesot que u^-1 = u^t.
El sumatorio de i a r es sigma_i * u_i * v_i, de todos los valores de i hasta r, proque despueés de R peude haber valroes nulos y por tanto, al haber 0s, nos da igual.
La mayor ignofmacion de A está en los valroes inciales y va tenidenco a 0.
S epeude hacer aproxmacioens de lso valoresd el suamorio, para A, por ejemplo los primos i valores, en lugar de TODOS ellos.



DESCENSO POR GRADIENTE utiliza el ERROR CUADRÁTICO

LA y con gorrito es la aproximacion.
LO que SE BUSCA: minimizar la distancia del punto a la linea de gradiente (sqrt(distancia de l punto a la y al cuadrado))
MINIMIZAR LA DISTANCIA ENTRE EL PHUNTO ORIGINAL Y EL APROXIMADO

SE suman todas las diferencias de los puntos (el real y el aprox), elevadas al cuadrado., Esto es:  SE PUEDE APLCIAR SIEMPRE. Con ajuste lineal y no lineal

DERIVADAS: al dereivar respeto a una variable, el resto de variables se toman como constantes (y si están solas, es normal que s evayan)
Si vamos a derivar f(x, y), respecto de x y luego de y, acada una de las derivadas que s ehagan prmeor son las DERIVADAS PARCIALES

VECTOR GRADIENTE: es el resultado de 'colocar' cada una de las respectivas derivadas parciales en un vector VERTICAL, tal que la primera derivada parcial estará arriba del todo y la ultima derivada aprcial abajo

HACIA DÖNDE DIRIGIR CADA UNA DE LAS ITERACIONES: es una funcion que nos dice, si miras en este punto, HAS DE DIRIGIRTE TANTOS MOVIEMIENTOS HACIA TAL DIRECCION (mas o menos)
- VECTOR GRADIENTE = direcciñon de máxio decrecimiento de f
VECTOR GRADIENTE = dirección de máximo crecimienot de f


METODO DEL GRADIENTE DESCENDIENTE: coger las medida siniciales que tenemos y restarle ALPHA por el vector gradiente ENTRE el normalizado del gradiente. Con lo que estaríamos restando el gradiente a un tamaño ALPHA que decidimos nosotros.
Si el valor dado es menor, hay que dirigirse, hacia 'MENOS'. UTILIZA EL ERROR CUADRÁTICO
PARTIMOS DE UN PUNTO SEMILLA


----------------------------------------
23/09/2025
SAMUEL
MODELOS INTERPETABLES: no son black boxes
- SOBREENTRENADO, si o o y a que porcentaje
- COMPARAR

Modelos lineales:

 con sumas y prodcutos de constatnes y variabels (y = ax + b es la más concida, fácil)
Se puede cambiar la x por sqrt de x, x cuadrado, lo que sea RELACIONADO CON LA X.
EJEMPLO: pOWER LAW Fehner, percepción es exponencial, (paso de la electricidad en el cerpo)

ESRTUDIAR , RESPECTO A LA GRÁFICA DE DATOS QUE S ENOS MUESTRA, CUÁL ES LA ECUACIÑÓN QUE MEJOR SE PLAICARIA, EJEMPLO, SI LOS PUNTOS SUGIEREN UNA LOGARITMICA, QUIZÁ ES MEJOR APLICAR UNA TRANSFORMACIÓN ANTES DE HACER EN EL MODELO. (perfcepcion del brillo es logaritmico, por ejemplo (70% se "ve" casi igualeq ue 100%))

DECIDIR COMO DIVIDR EL DATASET
Decidir, cuántos y cuáles (cuanto más complejo, más datos, 80 o más%, CON ESTE EJEMPLO, 50% ES BIEN // ALEATORIOOOO, intentar librar sesgos, HACER VARIAS VECES (varios modelos para asegurar que es consistente)) EN ESPACIO MULTIDIMENSIONAL ES MÁS COMPLICADO Y ÒR TANTO SE HACE PARTIICÓN ALEATORIA. 

- DECIDIR LA ECUACION Y CREAR, TANTAS ECUACIONES COMO DATOS TENGO (SISTEMA SOBREDETERMINADO)
- CONVERSIÓN  A MATRICIAL (COMO EN EL DIBUJO)
- y_E: datos de entrenamiento
- y_T: datos de test

Buscaremos el Min(Ax -b), donde Ax es mi modelo y b lo que debe predecir. (MINIMOS CUADRADOS, busca minimizar las distancias entre los dso vectores Ax y b).
x = (A^T * A)^-1 * A^t * b, tiene solución cuando se utiliza la norma ecuclidea.
LINEAL = Y = A*X + B  ----> x = (A^T * A)^-1 * A^t * b
CUADRÁTICA = Y = A*X^2 +B  ----> x = (A^2^T * A^2)^-1 * A^2^t * b

PARA PASAR A FORMA MATRICIAL: tenemos el vector en vertical [[a], [b], [c]] @ [[0.015^2, 0.015, 1],       = [50.5, 50.2, ]
                                                                               [0.015^2, 0.015, 1], ....] 


Error entrenamiento = sum((y - y_e)^2 de la primera componente, segunda, terfcera... ) == (y - y_e)(y - y_e)^t (por la trapsuesta)
Error validación = y - y_t

PARA HACER INTERPRETABLE EL ERROR CUADRÁTICO, SE DIVIDE ENTRE EL NÚMERO DE DATOS (Mean Squared Error, MSE)
SE OBTIENNE TODOS LOS ERRORES CUADRÁTICOS DE CADA MODELO

PARA DECIDIR SI HAY SOBREENTRENAMIENTO: comparar el error de entrenamiento con el error de validación. PAsa que el error para el conjunto de entrenamiento es más pequeño para el error de validación (una pequeña diferencia está bien (al finalsobre los datos entrenados hay mejor conocimiento), peor no puede se rmuy grande). 
COMPARAR: restar (diff absoluta), dividir (diff relativa)

Dividir: cercano a 1, son parecidos. muy cercano a 1,// no es problema, pero es raro //, cercano a 0: SOBREENTRENAMIETNO, por

Si se divide el error de entrenamiento entre el error cuadrado (ambos elevados al cuadrado, por tanto CHI-CUADRADO), es una distirbución de Fischer, SE PUEDE CALCUALAR EN QUE PERCENTIL DE LA DSITRIBUCIÓN ESTÁ!!! SI 55, OK EN EL CENTRO, SI 5 O 95, ESTÁ EN UNE EXTREMO.




FALTA EL ANÁLISIS FINAL

SOBREENTRANDO: CUANDO LA RELACIÓN DE PARÁMETROS RESPECTO A DATOS ES DEMASIADO ALTA (demasiados x^2, etc... para tan pocos datos)
-----------------------------------------------------
¿CUÁNDO UNA VARIABLE ES IMPORTANTE?  -> MODELO CON Y SIN VARIABLE, si al 95% de confianza son ..., la variable es importante, si sale un PERCENTIL MEDIO, es que da igaul
INTUIR VARIABLES:
- COmbiacion lineal de las variables (sumar y multiplicar las variables por componentes O ENTRE ELLAS..) t = ax + by +cz +wd). Podemos aplicar TRANSOFMRACIONES LINEALES a variables que consideremos y seguirá siendo combinación lineal. Multiplicar xy es un factor no lineal pero en el modelo es lineal todavía.
- en c = a*p +b*h +c: para saber que variable es mas improtante hay que NORMALIZAR. (dos técnicsas)(1, restar dividri máximo tiene valores de 0 a 1, PERO muy sensible a errores, pues todo se centra en max o minimo, SI ES OUTLIER O ESTÁ MEDIDO MAL, PROBLEM) (2, rstar y divividr en std, valores CENTRADOS, más robusto,, "z-score normalization", entre -1, 1 el 60, ... bastabte centrado, AUNQUE NO HAY LIMITES DE VALORES, pensda para TRANSOFMRAR UNA MEDIA DE NORMAL A .. ---> SI LOS DATOS NO TIENEN DISTRIBUCIÓN NORMAL...)

SISTEMA DE ECUACIONES SOBREDETERMINADO CON FORMA MATRICIAL (Ax = b) ---- > [val1, val2] [a b c] = [temp] (EN COLUMNAS)

- Como la presión es muy baja: O LA RELACION ENTRE PRESION Y HUMEDAD NO ES LINEAL o LA RPESION NO ES IMPORTANTE
# Si sale cerca del 50%, son similares
# Si sale muy alto o muy bajo, HAY diferenciar significativas entre los modelos (2.5-97.5)

MODELOS DINÁMICOS:
Utilizar datos del pasado para predecri los del futuro



---------------------------------------------------------------------
29/09/2025 Maria
--------------------------------------------------
definición de probabilidad = casos favorables / casos posibles (toda P entre 0 y 1)

discreta = finito, personas hay 1,2,3, no 1.2
		 suma de valores da 1
continua = hay infintos resultados entre uno y otro
                  se hace la integral (sumatorio de inifnitos valores)


population = no conozco todos los elementos, por tanto es un PARAMETRO aproximado
sample = conozco seguro, por tanto son ESTADÍSTICAS


LA medai devuelve el valor, pero no tiene que ser uno de los casos reales (por las dsicretas, por ejemplo, ejemplo de los dados que se tiran cuya media es 3.5 aunque el dado no tenga una cara de 3.5).


Comparativa de los años: la varianza es la misma aunque crezcamos pasado un año, pero la medai habrá crecido la parte porporcioanr del tiempo que haya transcuirrdo.

Benrouilli: probabilida de que sea o A o no A y punto.
          -  la suma de Bernoulli conduce a distirbucion binomial
          -  se mantiene constate la probabilidad del suceso
X - Bin(n, p): dsitribucion binomial uq edepemde de n y de p

---------
App = Probability Distributions, allows to select fro probability distributions.
-------



---------------------------------------------------------------------
01/10/2025: Conjoint Probability
---------------------------------------------------------------------


Varianza x: S_x^2 = sum(x_i - x.mean)/(n - 1)
Varianza y: S_y^2 = sum(...)/(n - 1)          -------> n son los pares de datos
Covarinza es el grado de dispersión entre ambasa variables (igual que la Varianza de una pero con dos)

Cob(x, y) = E((x_i - mu_x)(y_i - mu_y))
           E = expected value


Misma variablidadad cambia con unidades diferentes. Peso yh altura, en kg, g, m, cm... --> Problema de dependencia de unidades de trabajo (muy relacioenadas). Para ver la relacion hay que gacer el correlaction coefficient. --> normalizar valroes . Da relacion LINEAL entre variables (no relaicón lineal NO SIGNIFICA NO RELACIÓN). --> Variables relacioneadas por ejemplo cuadráticamente recibiran un coeficiente 0 porque no estará relacionado, A PESAR DE TENER UNA RELACIÓN CUADRÁTICA.

Ejercicio:
P(X = -1, Y = 1) = 1/3, P(X = 0, Y = 0) = 1/3, P(X = 1, Y = 1) = 3

# LA X es discreta, porque toma valores conocides, por lo que E = sum(valor * Pobabilidad_i) 
# RECUERDA: CASOS FAVORABLES / CASOS POSIBLES

Cov(X, Y) = E[X Y] - E[X]*E[Y] = 0                           		#
E[X] = -1*P(X=-1) + 0*P(X=0) + 1*P(X=1) = -1/3 + 0 + 1/3 = 0   		# Expected value de X
E[Y] = 0*P(Y = 0) + 1*P(Y = 1) = 0 + 1*(1/3 + 1/3) = 2/3                # Expected value de Y  
E[X Y] = E[X * X^2] = E[X^3] = (-1)^3*P(X = -1) + ... resto * prob = 0  # Expected value of XY                       

Como la Cov(X, Y) = 0 --> Cor(X, Y) = 0

PAra ver si son indeoendeintes: Probabilidad conjunta == Producto de Probabilidades ---> Ha de cumplirse para todos los casos!!

P(X=-1, Y=1) == P(X=-1)*P(Y=1) -> 1/3 =/= 1/3*2/3




----------
Sample: muestra pequeña de la población ---> para inferir datos de la población 
Descriptiva: de la muestra, Inferenical, de la población.

Mu es la medai de la poblaciñon y x^_ es la medai de una muestra
sigma^2 es varianza población, S^2 de muestra
sd es s en muestra y sigma en población

Mediana: deja la mitad de los datos por arriba y por debajo
Skewness: …
Fructosis: mide si los datos son más 'chatos' o 'picudos' que la normal.

Quartiles: divide los datos en 4 partes iguales (SON LA DIVISIÓN: 25, 50, 75, SÓLO 3 'CORTES')
Quantiles: n-cortes, percentil --> 99 cortes (sobre 100, %).

IQR: Q3 -Q1
length bigote = Q_i - 1.5xIQR (si esto es > que el ultimo valor, llega sólo hasta ekl ultimo valor).




1.96 ---> intervlo de confianza del 95%, cambiarlo cambia el intervalod e confianza, que tñu decides cuándo ueires más o menos confianza (más confianza == menos "errores" pero menos orecision.)
Cuanto más grande el tmañaod e muestra, más se acerca al intervalo.

---------------------------------------
EJERCICIO INTERVALO 1

n     = 15 ejecuciones
x^_   = 47.07 ms
s^2   = 2.97 ms^2
mu    = 47.8 ms     (¿?)
alpha = 0.05 --> 1.95

¿Mu = 47.8 con intervalo de 1.95?

IC(95%) = [x^_-t_(1-alpha/2)*s/sqrt(n)]
        = [47.07 - 1.761 * sqrt(2.97)/sqrt(15)]   # sacar con el alfa y y los grados de libertad el t_(...)
        = [46.1156, 48.0243]                      # Izqd y dcha

---------------------------------------
EJERCICIO INTERVALO 2

n     = 100 ejecuciones
x^_   = 778
s^2   = 193.4
sigma =                                           # Como no tengo sigma utilizo t, que se aproximará a la normal, haré normal con s, porque n es muy grande, pero no sigma
alpha = 0.05 --> 1.95

IC(95%) = [x^_-t_(1-alpha/2)*s/sqrt(n)]
        = [778 - 1.96 * sqrt(193.4)/sqrt(100)]   # sacar con el alfa y y los grados de libertad el t_(...)
        = [46.1156, 48.0243]                       # Izqd y dcha



---------------------------------------
EJERCICIO CHI-CUADRADO

pblacion = votantes
muestra = 200vot
variables:                                # Queremos comparar y ver si son independientes o no
- rango de edad
- partido de voto     --------> tabla de frecuencias observadas (O_ij)

H_0 = que sean indeoendeitnes
H_1 = que no sean independientes
Alpha = 0.05

E = O_i * O_j / muestra ----> tabla de frecuencias esperadas (E_ij)

`X` = sum(sum(O_ij - E_ij) / E_ij)                       # sumatoria de cada categoría (2 sumatorios en este caso)
grado de libertad = (cat_i - 1) * (cat_2 - 1)            # Obtener los grados de libertad con las categorías         

REAHCZAR = NO ESTAR EN EL ALPHA (EN EL INTERVALO DE CONFIANZA)






----------------------------
03/10/25
----------------------
ARBOLES DE DECISIÓN: decidir de entre todas las variables, la primera y a partir de esas las demás en importancia.

ENTROPÑIA: Homogeneidad entre los datos (con logaritmos para aproximar, con given formula)
con la misma diferencia, pero mñas datos hay mas homgeneidad.

GANANCIA INFORMACION: ENTROPIA - sum()   ->< elegir nodo primero según amyor ganancia
ELEGIR el siguiente nodo dentro de los nodos del primero viendo como les afecta el resto de variables.


----------------------------------
13/10/25
Aplicación the Supervised Model Classification
----------------------------------
Regresión: de variables ¿?, se predice categorías
Clasificación: de variables descriptivas, se predice categorías

LA MAYORÍA SON EJEMPLOS DE CLASIFICACIÓN

Modelos eneseñados: son los REPRESENTANTES DE LAS FAMÍLIAS: DISTANCIAS, PROBABILIDAD (Bayes es POBRE), REGRESIÓN LOGÍSTICA, ÁRBOLES DE DEICISIÓN, REDES NEURONALES ARTIFICIALES (MÁS ADELANTE, 1 NEURONA, MULTIPLES NEURONEAS, CAPAS INTERNAS, ...), MÁQUINA DE VECTOR DE SOPORTE, estado del arte ------> para datos tabulares: RANDOM FOREST Y XBOOST (son ensamble: conjunto de modelos)

- REGRESIÓN LINEAL: 
- REGRESIÓN LOGÍSTICA: 
- EJERCICIO: solver en LLinear REgression object, depende de los datos
            ajustar
               predice (devuelve las clases)
		confusion matrix --> HARD WAY (manually select from matrix)
                                 |-> EASY WAY
	predict_proba(devuleve vector de pertenencia de clase para cada test)
	
	cambia el thresold y calcula otra vez la confusion matrix (lanza runa curva Roth y comprobar el mejor cutoff)

- DECISION TREE: sencillos de entender, robustos para NaN, 
		ESCOGER EL PRIMER ATRIBUTO EN LOS CONJUNTOS MÁS PUROS (según gananxia, entropía)
		¡¡¡¡son muy dependeintes del datatset de entrenamiento!!!!
		CRITERIO DE PARADA
		NODOS HOJA
		NODO PURO
		PUEDE HABER ATRIBUTOS IRRELEVANTES
		SE PUEDE TRANSOFMRAR DE ARBOL DE DECISION A REGLAS
		SI EL NODO FINAL NO ES NODO PURO (UNA SOLA CLASE), SE PARA POR OTRO CRITERIO (profundidad máxima, por ejemplo, ) -> CLASE MAYORITARIA
		SCIKIT, NO TRABAJ CON DTOS CATEG´ROCOS: LOS PASA ONE-HOT/ NUMEROS
		GRÄFICA CON PARALELAS = ARBOL DE DECISION, del cual se podría dibujar el árbol: mira la partición "MÁS GRANDE" y luego las sucesivas más pequeñas.
		OVERFITTING, A MÁS DEPTH MEJOR PERO NO GENERALIZA BINE PORQU EHEMMEMORIZADO TODO. --> GRID SEAARCH PARA VER LA MEJOR PROFUNDIDAD
		

- EJERCICIO (TITANIC):
		DISCRETIZACIÓN DE VARIABLES (edad): DEBE DE SER LÓGICA

- KNN: MODELO lazy (not eager), BASADO EN DISTANCIAS 
		VOY INCREMENTADNO K (1,2,3,4,...) Y SE VA COMPARANDO CON OTROS MODELOS.
		+ k = + generalidad y fronteras más rectas (pero con k más baja (1-9), suele dar valores correctos también)
		 EJEMPLOS MEZCLADOS Y MUY PEGADOS A LA FRONTERA DE DECISIÓN SON MUY MUY DIFÍCIL DE PREDECIR (INSTANT HARNESS, PSICOLGY, ...)

- SVM ALGTH (support vector machine): BUSCA LA MEJOR FRONTERA LINEAL (MEJOR HIPERPLANO, EN MUCHAS DIMENSIONES) CON UN MARGEN MÁXIMO CON VECTORES DE SOPORTE. SI LOS DATOS NO SON SEPARABLES --> KERNEL TRICK. 

EJERCICIO: 
		HACEMOS DROP DE LAS COLUMNAS PERO LUEGO LAS UTILIZAREMOS PARA MEJORAR EL ACCURACY


-------------------------------------
14/10/25
-------------------------------------
- ENSEMBLE ALGORITHMS: combinación de algoritmos
		HISTORIA: Wisdom of the crowds (i.e. mean of everyone's knowledge about cows weight).
		SINGLE: 1 model
		VOTING: 
		BAGGING: random forest (choose number of trees + try to have DIFFERENT TREES (diif information))
		BOOSTING: XGBoost, next model tries to solve earliers error, same with the following, and so on.

- VOTING: 1 training set, SEVERAL DIFF CLASSIFICATION MODELS at the same time, hard/soft voting (hard: majority votes, soft: average probability class) in the example we can see how 0s have a lot more confidence than to 1s. Soft vote is more recommended. 

BEst models at final Project end up being a voting ensamble ->
		-> alpha*RF+beta*XGBoost+Thet*LAsso=1

- BAGGING (Bootstrap AGGregatING): they use mostly decisión trees (problema that with the same data, they Will always give the same decisión tree, so we need them to be INDEPENDENT and not to CORRELTATE BETWEEN THEM)  -> DIVERSITY. (ALEATORIEDAD FUNCIONA MEJOR)
	-> BOOTSTRAP SAMPLING: muestreo con repetición del mismo tamaño que el inciial (algunos elementos están repetidos, para tener el mismo tamaño que el inicial). ESTÁ DEMOSTRADO EMPÍRICAMENTE QUE A MÁS ÁRBOLES, MEJOR RESULTADO.
	-> recoradmos que son muy dependientes deldataset de entrenamiento, con lo que hay mas varibailidad (los que más se repiten tienen más relevancia --> APRENDIZAJE BASADO EN COSTES)
	-> FEATURE RANDOMNESS: …
	-> PERMITE PARALELIZAR MUCHOOOOO (son independientes)
	-> SON MODELOS QUE GANAN MUCHAS COMPETIICONES

¿¿¿manipular el sampling para buscar algún tipo de diversidad/ aleatoriedad concreta se hace? Este caso no explora todas las posibilidadesdel problema? ???

- BOOSTING: en cascada, van comprobando los errores del anterior.
	-> Assigns weights to errors in every SEQUNECE (THEY ARE sequential).
	-> ADA BOOST: increases inaccurately classified data automatically (such us in DT) in each iteration and at the end combines every weight.
	-> GRADIENT BOOSTING: tries to predict erros (minimize previous model errors). XGBOOSTING: mpstly used, not in scikit-learn, very costly (because of stable/balanced tres with not too deep branches, which are the best, but manipulating them is costly). LIGTHGBM are less costly



-----------------
Unsupervised: grouping and RULES
	-> Grouping: number of groups is hyperparameter (decided from DENDOGRAM)
- HIERARCHICAL:
	-> AGG-CLUSTERING: BTM->UP, first iteration: instances are cluster, second: closest ones are cluster-> closest clusters e cluster.
	-> DIVISIE CLUSTERING: TOP->DOWN
	-> WE DECIDE WHEN TO "CUT" THE CLUSTER
	-> DISTANCES: DIFFERENCES OF VECTORS (INTER-CLUSTER DISTANCES), WHO DEPENDE PN THE DISTIRBUTION OF THE DATA (WE CREATE ALL THE DISTANCES AND DECIDE ANNALLYTICALLY)

- NON-HIERARCHICHAL:


--------------------------------------------------
16/10/25 PROJECTO ML
--------------------------------------------------

- Overall QUality --> num vs. discrete
- Change to numerical
- NA detected as NaN: but for a basement it is not, jsut has no basement. CAREFUL
- DATA EXPLORATION: important
- No nº of baths but nº of baths in every floor: créate a new one.
- ¿? Look for refrences in housing webs, to see what people look for ¿?
- 80 variables input y 1 output (1500x81)
- Apply changes to train also to test (81 var at train, 80 at test), if f(x_1) to train, aslo to test. (COLUMNS ESPECIALLY, ROWS ONLY IN TEST)
- Check Code and Discussions
- fmartinez
- Menos variables y más lanzar modelos, muchos
- Lasso, Ridge, (pues es normal tener overfitting)









		






	